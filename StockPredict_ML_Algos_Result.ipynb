{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsjkLd5hBAus"
      },
      "source": [
        "# **Machine Learning Implentation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYD1vIMR_2Wy"
      },
      "source": [
        "**Installing Spark Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wErI8lZsB29s",
        "outputId": "1aee89d1-e9c2-4e61-c340-c9e4296d97b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=8db88fa5b129e8ed9676776d7d6d2748e8707e81f3f97285c688081d57902f53\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KHE94cz0_ws",
        "outputId": "dd3f68f8-4c30-4b10-b152-12450f094b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bloom-filter2\n",
            "  Downloading bloom-filter2-2.0.0-1.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Discarding \u001b[4;34mhttps://files.pythonhosted.org/packages/2b/5d/8de4a849ebe212217e6d8f4798a6918d4035e741c44730da81272f170b47/bloom-filter2-2.0.0-1.tar.gz (from https://pypi.org/simple/bloom-filter2/)\u001b[0m: \u001b[33mRequested bloom-filter2 from https://files.pythonhosted.org/packages/2b/5d/8de4a849ebe212217e6d8f4798a6918d4035e741c44730da81272f170b47/bloom-filter2-2.0.0-1.tar.gz has inconsistent version: expected '2.0.0.post1', but metadata has '2.0.0'\u001b[0m\n",
            "  Downloading bloom_filter2-2.0.0-py3-none-any.whl (6.8 kB)\n",
            "Installing collected packages: bloom-filter2\n",
            "Successfully installed bloom-filter2-2.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install bloom-filter2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elxt4G7pME6u"
      },
      "source": [
        "** Import required libraries for the project**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "_znrd8GkLs8P"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lower, udf, concat_ws, concat, to_date, collect_list, translate, regexp_replace, when\n",
        "from pyspark.sql.types import BooleanType, StringType\n",
        "from bloom_filter2 import BloomFilter\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import NaiveBayes, LogisticRegression, RandomForestClassifier, DecisionTreeClassifier, LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRbVgvlnBJ89"
      },
      "source": [
        "**Instantiate a Spark Session**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "bT8EN4LsBPwV"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName('SentimentAnalyzer').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlBE8YwrAAXg"
      },
      "source": [
        "**Loading Reddit Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Ac-LOCYCBSUo"
      },
      "outputs": [],
      "source": [
        "# Load data and rename column\n",
        "df = spark.read.format(\"csv\")\\\n",
        "    .option(\"header\", \"true\")\\\n",
        "    .option(\"inferSchema\", \"true\")\\\n",
        "    .option(\"multiLine\", \"true\")\\\n",
        "    .option(\"delimiter\", \"¥\")\\\n",
        "    .load(\"/content/drive/MyDrive/BigDataProject/reddit_data1.csv\")\\\n",
        "    .coalesce(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt3IkWXfBN0N",
        "outputId": "e8b006fe-d2e5-475d-b470-346092eba368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbDiAeFs51PX",
        "outputId": "70e40c48-362c-415c-8503-d84ae1f50ba7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Row(_c0='0', id='rtbwo0', title='Happy New Years you retards❤️', comment=None, timestamp='2021-12-31 23:57:11', time_key=datetime.date(2021, 12, 31), SP500=-0.0026261799136575448, TESLA=-0.012668840823859995)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0YZghblLs8S"
      },
      "source": [
        "**Preprocessing data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Rje0VFBIb6Dr"
      },
      "outputs": [],
      "source": [
        "df = df.withColumn('comment', lower(col('comment')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "S1-wEOr9b6Ha"
      },
      "outputs": [],
      "source": [
        "# filter to see if title column contains any keyword from keywords\n",
        "keywords = [\"SP500\" , \"S&P500\"]\n",
        "def my_filter(col):\n",
        "    for keyword in keywords:\n",
        "        if keyword.lower() in col.lower():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "filterUDF = udf(my_filter, BooleanType())\n",
        "ids = df.filter(col(\"title\").isNotNull()).filter(filterUDF('title')).select(\"ID\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "T_-J7mgUb6LP"
      },
      "outputs": [],
      "source": [
        "# create and populate bloom filter\n",
        "bloomFilterIDS = BloomFilter(ids.count(), 0.000000001)\n",
        "collected_ids = ids.collect()\n",
        "for row in collected_ids:\n",
        "    bloomFilterIDS.add(row[\"ID\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "odMUrmNPcPUE"
      },
      "outputs": [],
      "source": [
        "broadcastFilterIds = spark.sparkContext.broadcast(bloomFilterIDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "yVqQ0kN8cTAY"
      },
      "outputs": [],
      "source": [
        "def my_filter_by_ids(col):\n",
        "    return col in broadcastFilterIds.value\n",
        "        \n",
        "filterIdUDF = udf(my_filter_by_ids, BooleanType())\n",
        "bloomedFilteredData = df.filter(col(\"SP500\").isNotNull()).filter(filterIdUDF('ID'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "9rC8yXlYxdMP"
      },
      "outputs": [],
      "source": [
        "bloomedFilteredData = bloomedFilteredData.withColumn(\"date_stock\",to_date(\"timestamp\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "HcXqIQIyzYcK"
      },
      "outputs": [],
      "source": [
        "bloomedFilteredData = bloomedFilteredData.na.drop(subset=[\"comment\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "PiIlqF3Y7A_w"
      },
      "outputs": [],
      "source": [
        "bloomedFilteredData= bloomedFilteredData.drop(\"_c0\",\"id\",\"title\", \"timestamp\", \"time_key\", \"TESLA\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Z9EI00oMdyfM"
      },
      "outputs": [],
      "source": [
        "df1 = bloomedFilteredData.groupby('date_stock', 'SP500').agg(collect_list('comment').alias(\"comment\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "bO9UBnC7DG_I"
      },
      "outputs": [],
      "source": [
        "df2 = df1.withColumn(\"comment\",\n",
        "   concat_ws(\",\",col(\"comment\")))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "wDm_lBcgCYcH"
      },
      "outputs": [],
      "source": [
        "df2 = df2.withColumn('comment', translate('comment', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~', '\" '))\n",
        "df2 = df2.withColumn('comment', regexp_replace('comment', '\"', ' '))\n",
        "df2 = df2.withColumn('comment', regexp_replace('comment', \"'\", ' '))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti1eQFlpFNka",
        "outputId": "3f2d9e7b-bb75-49ff-e61f-284b91460d65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|date_stock|SP500              |comment                                                                                                                                                                                                                                                                                             |\n",
            "+----------+-------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|2022-05-04|0.02986242108440229|hey dangmemesub please submit your post as a text post and add some additional context make sure to include the link \\n\\ni am a bot and this action was performed automatically please contact the moderators of this subredditmessagecomposetorwallstreetbets if you have any questions or concerns|\n",
            "+----------+-------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2.filter(df2.date_stock == \"2022-05-04\") \\\n",
        "    .show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlW3vz2N2HQX",
        "outputId": "65763023-2c0a-400c-e7cc-35948ff8b7d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Row(date_stock=datetime.date(2022, 5, 4), SP500=0.02986242108440229, comment='hey dangmemesub please submit your post as a text post and add some additional context make sure to include the link \\n\\ni am a bot and this action was performed automatically please contact the moderators of this subredditmessagecomposetorwallstreetbets if you have any questions or concerns')"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Z62LvOV6HBdb"
      },
      "outputs": [],
      "source": [
        "df2= df2.withColumn(\"SP500\", when(df2[\"SP500\"]>0,1).otherwise(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "-RUU5e5pKSGr"
      },
      "outputs": [],
      "source": [
        "df2= df2.withColumnRenamed(\"SP500\",\"label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miGOeAEQMTW9"
      },
      "source": [
        "**Spark ML pipeline setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2zEllvHQk0G",
        "outputId": "ce01fb4f-3b75-48e6-a4f3-85f7e5db8aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " RegexTokenizer_93917d7946bd\n",
            "\n",
            " StopWordsRemover_1e0879e44c61\n",
            "\n",
            " CountVectorizer_2518b940c172\n",
            "\n",
            " VectorAssembler_d7e18903be77\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[None, None, None, None]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stages = []\n",
        "\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"comment\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
        "stages += [regexTokenizer]\n",
        "\n",
        "swr = StopWordsRemover(inputCol=\"tokens\", outputCol=\"Comments\")\n",
        "stages += [swr]\n",
        "\n",
        "cv = CountVectorizer(inputCol=\"Comments\", outputCol=\"token_features\", minDF=2.0)#, vocabSize=3, minDF=2.0\n",
        "stages += [cv]\n",
        "\n",
        "\n",
        "vecAssembler = VectorAssembler(inputCols=['token_features'], outputCol=\"features\")\n",
        "stages += [vecAssembler]\n",
        "\n",
        "[print('\\n', stage) for stage in stages]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQzn-A51Ls8W"
      },
      "source": [
        "**Training and testing models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwAdXy22oGYl"
      },
      "source": [
        "##Pipeline Fitting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "jEoeLrc2oSRs"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(stages=stages)\n",
        "data = pipeline.fit(df2).transform(df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "6egja6DLLs8W"
      },
      "outputs": [],
      "source": [
        "train, test = data.randomSplit([0.7, 0.3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzg_LXEBMatD"
      },
      "source": [
        "## Naive Bayes Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Jp8SQE7LtIVt"
      },
      "outputs": [],
      "source": [
        "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
        "model = nb.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF1hgaEs63ng",
        "outputId": "b4227d75-7009-4f21-f89e-a13969480c78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----------+-------------------------------------------+\n",
            "|label|prediction|probability                                |\n",
            "+-----+----------+-------------------------------------------+\n",
            "|1    |1.0       |[2.0421381807012315E-12,0.9999999999979579]|\n",
            "|0    |1.0       |[2.046222685002206E-11,0.9999999999795377] |\n",
            "|0    |0.0       |[0.9917582109998717,0.00824178900012843]   |\n",
            "|0    |1.0       |[0.2525753840628213,0.7474246159371786]    |\n",
            "|0    |1.0       |[6.6625734958572395E-21,1.0]               |\n",
            "|0    |1.0       |[2.972663531775293E-13,0.9999999999997027] |\n",
            "|0    |0.0       |[1.0,6.7886360035995E-105]                 |\n",
            "|0    |0.0       |[0.9999530717089651,4.692829103496594E-5]  |\n",
            "|0    |0.0       |[0.9938682673666384,0.006131732633361661]  |\n",
            "|0    |0.0       |[0.9999994773183344,5.226816657378221E-7]  |\n",
            "|0    |1.0       |[5.015124412461461E-13,0.9999999999994984] |\n",
            "+-----+----------+-------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions = model.transform(test)\n",
        "# Select results to view\n",
        "predictions.limit(20).select(\"label\", \"prediction\", \"probability\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjJfLr_I7x4c",
        "outputId": "435609c6-1d27-4cb6-f1a7-53a00b9d3227"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Area Under ROC:  0.75\n"
          ]
        }
      ],
      "source": [
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
        "nbaccuracy = evaluator.evaluate(predictions)\n",
        "print (\"Test Area Under ROC: \", nbaccuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMB5x791Mgl1"
      },
      "source": [
        "**Cross Validation Evaluation for Naive Bayes Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO_SrTyy8O1q",
        "outputId": "bb1dd660-3ff8-47d6-d981-9fefe0c3f18c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.4"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Create ParamGrid and Evaluator for Cross Validation\n",
        "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0]).build()\n",
        "cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
        "# Run Cross-validation\n",
        "cv = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
        "cvModel = cv.fit(train)\n",
        "# Make predictions on testData. cvModel uses the bestModel.\n",
        "cvPredictions = cvModel.transform(test)\n",
        "# Evaluate bestModel found from Cross Validation\n",
        "evaluator.evaluate(cvPredictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlBBcrJE8P27",
        "outputId": "73e5f9cf-b829-4674-cd0c-b13a4fa365c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "[Stage 492:>                                                        (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Area Under ROC:  0.8333333333333334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "\n",
        "# Make predictions on testData. cvModel uses the bestModel.\n",
        "cvPredictions = cvModel.transform(test)\n",
        "# Evaluate bestModel found from Cross Validation\n",
        "print (\"Test Area Under ROC: \", evaluator.evaluate(cvPredictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc19OnAkMoxw"
      },
      "source": [
        "## Logistic regression Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J32nsys_0sh",
        "outputId": "bf1cd845-b07c-44ee-e79d-44b990d6f896"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5\n"
          ]
        }
      ],
      "source": [
        "log_reg = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
        "model2 = log_reg.fit(train)\n",
        "predictions = model2.transform(test)\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator().setLabelCol('label').setRawPredictionCol('prediction').setMetricName('areaUnderROC')\n",
        "lgaccuracy = evaluator.evaluate(predictions)\n",
        "print(lgaccuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKZ52e7DMu3K"
      },
      "source": [
        "**Cross Validation Evaluation for logistic Rergression Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hb88QAexc0G",
        "outputId": "bf8cbccc-120a-4200-d9db-c29a8b9cbc30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create ParamGrid and Evaluator for Cross Validation\n",
        "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0]).build()\n",
        "cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
        "# Run Cross-validation\n",
        "cv = CrossValidator(estimator=log_reg, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
        "cvModel = cv.fit(train)\n",
        "# Make predictions on testData. cvModel uses the bestModel.\n",
        "cvPredictions = cvModel.transform(test)\n",
        "# Evaluate bestModel found from Cross Validation\n",
        "evaluator.evaluate(cvPredictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvWl7GAJM3fT"
      },
      "source": [
        "## Random Forest Classifier Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsrsrYTuDch7",
        "outputId": "29f3c1d5-cb39-44e0-bed7-a5987e1765c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/12/22 17:25:03 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 22 (= number of training instances)\n",
            "[Stage 1972:>                                                       (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5833333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier().setLabelCol('label').setFeaturesCol('features').setNumTrees(10)\n",
        "model = rf.fit(train)\n",
        "predictions = model.transform(test)\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator().setLabelCol('label').setRawPredictionCol('prediction').setMetricName(\"areaUnderROC\")\n",
        "rfaccuracy = evaluator.evaluate(predictions)\n",
        "print(rfaccuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qEwUCfYM9yp"
      },
      "source": [
        "**Cross Validation Evaluation for Randon Forest Classifier Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loPyo-4pxi57",
        "outputId": "e0ffb0bc-5498-428e-bd9f-3c1c026803db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/12/22 17:25:25 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:25:37 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:25:42 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:25:46 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:25:51 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:25:55 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:25:59 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:26:03 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:26:15 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:26:28 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:26:33 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:26:38 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:26:43 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:26:47 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:26:52 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:26:56 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:27:09 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:27:21 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:27:26 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:27:30 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:27:35 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:27:39 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:27:44 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:27:49 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:28:09 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 22 (= number of training instances)\n",
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5833333333333333"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create ParamGrid and Evaluator for Cross Validation\n",
        "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0]).build()\n",
        "cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
        "# Run Cross-validation\n",
        "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
        "cvModel = cv.fit(train)\n",
        "# Make predictions on testData. cvModel uses the bestModel.\n",
        "cvPredictions = cvModel.transform(test)\n",
        "# Evaluate bestModel found from Cross Validation\n",
        "evaluator.evaluate(cvPredictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xBL_nh1NFYr"
      },
      "source": [
        "## Decision Tree Classifier Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c-BNl8ToHBW",
        "outputId": "4ab35c96-72f5-467c-bbaf-c4b50b8afaac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/12/22 17:28:39 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 22 (= number of training instances)\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        }
      ],
      "source": [
        "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
        "dtModel = dt.fit(train)\n",
        "predictions = dtModel.transform(test)\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator().setRawPredictionCol('prediction')\n",
        "#evaluator = BinaryClassificationEvaluator(labelCol=\"label\", featuresCol=\"features\", maxDepth=2)\n",
        "dtAccuracy = evaluator.evaluate(predictions)\n",
        "print(dtAccuracy) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM8CPOcINJqh"
      },
      "source": [
        "**Cross Validation Evaluation for Decision Tree Clasifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grX1vvG_xlT8",
        "outputId": "f50b387f-753e-43a9-a9c9-aec3e40f03da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/12/22 17:29:00 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:29:13 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:29:18 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:29:22 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:29:26 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:29:31 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:29:37 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:29:39 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
            "22/12/22 17:29:50 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:30:01 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:30:03 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:30:06 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:30:09 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:30:11 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:30:14 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:30:17 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 15 (= number of training instances)\n",
            "22/12/22 17:30:27 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:30:38 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:30:40 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:30:43 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:30:45 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:30:48 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:30:50 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:30:52 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n",
            "22/12/22 17:31:11 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 22 (= number of training instances)\n",
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create ParamGrid and Evaluator for Cross Validation\n",
        "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0]).build()\n",
        "cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
        "# Run Cross-validation\n",
        "cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
        "cvModel = cv.fit(train)\n",
        "# Make predictions on testData. cvModel uses the bestModel.\n",
        "cvPredictions = cvModel.transform(test)\n",
        "# Evaluate bestModel found from Cross Validation\n",
        "evaluator.evaluate(cvPredictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0fTSyE-NR2y"
      },
      "source": [
        "## Support Vector Classifier Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hzbUUxWyJwN",
        "outputId": "4ddbd5bf-4dad-4c51-9516-172869af9c7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 4252:>                                                       (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy = 0.75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Define your classifier\n",
        "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
        "\n",
        "# Fit the model\n",
        "lsvcModel = lsvc.fit(train)\n",
        "\n",
        "# Compute predictions for test data\n",
        "predictions = lsvcModel.transform(test)\n",
        "\n",
        "# Define the evaluator method with the corresponding metric and compute the classification error on test data\n",
        "evaluator = BinaryClassificationEvaluator().setRawPredictionCol('prediction')\n",
        "svmaccuracy = evaluator.evaluate(predictions) \n",
        "\n",
        "# Show the accuracy\n",
        "print(\"Test accuracy = %g\" % (svmaccuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8GY5s6UNWT4"
      },
      "source": [
        "**Cross Validation Evaluation of Support Vector Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_FoTToNxpud",
        "outputId": "1211b28c-968b-4136-b5e2-97d51059f1b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create ParamGrid and Evaluator for Cross Validation\n",
        "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0]).build()\n",
        "cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
        "# Run Cross-validation\n",
        "cv = CrossValidator(estimator=lsvc, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
        "cvModel = cv.fit(train)\n",
        "# Make predictions on testData. cvModel uses the bestModel.\n",
        "cvPredictions = cvModel.transform(test)\n",
        "# Evaluate bestModel found from Cross Validation\n",
        "evaluator.evaluate(cvPredictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
